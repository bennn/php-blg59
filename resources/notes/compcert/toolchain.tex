\section{Verified Toolchain}

Compiler correctness from CompCert C to Assembly is a terrific achievement, but only the first milestone for the CompCert team.
The original release did not support \lstinline{goto} statements or union types, but these have been added over the years.


\subsection{cchecklink}
\label{checklink}

The {\tt cchecklink} tool validates the results of assembling and linking on the PowerPC EABI and ELF/SVR4 platforms~\cite{refman}.
It works by comparing {\tt .sdump} files generated by CompCert against the linker's executable.
Every variable and function in the assembly code is matched against a code segment in the executable.

{\tt cchecklink} does not claim to prove anything, but provides ``an array of sanity checks for making sure the assembler/linker do not perform unsafe late-stage optimizations''.
This is weak assurance, but certainly better than having nothing at all (as is the case on ARM or \intel).

The {\tt checklink} sources are online in Robbert Krebber's mirror,\footnote{\url{https://github.com/robbertkrebbers/compcert/tree/master/checklink}} but not in the {\tt AbsInt} repo.
Besides the reference manual~\cite{refman}, {\tt checklink} is undocumented.


\subsection{Floating Point Arithmetic}
CompCert includes a complete formalization of IEEE-754 floating-point arithmetic~\cite{ieee-754,bjlm-floating}.
From a users perspective, this means that floating point operations are essentially done with infinite precision and rounded when stored to variables\textemdash that is the IEEE specification.
This guarantee is impressive because it holds for all three of CompCert's back-ends, regardless of whether the underlying machine is 32-bit or 64-bit.

In a lesser compiler, floating point numbers might be represented with double precision (using 64-bit numbers with 53-bit precision) or extended precision (80-bit numbers, 64-bit precision) \emph{depending on the underlying machine}.
At first glance, this should not be a problem because the C standard dictates that \lstinline{float} variables use 32 bits and \lstinline{double} variables use 64 bits, but there are two under-specified points.
For one, the \lstinline{long double} type uses a machine-dependent amount of precision.
Second, it is not clear where to round variables in the middle of a compound expression.
There are machine-specific and compiler-specific opportunities to get different results out of the same operation.

CompCert's solutions are to:
\begin{itemize}
\item Ban the \lstinline{long double} type, allowing it only as a synonym for \lstinline{double}.
\item Never use extra precision bits.
\item Round intermediate operations to the more-precise operand
\end{itemize}
Additionally, CompCert will never reorder instructions to reduce the number of multiply or add operations in a term.
Instead it offers a library of ``fast math'' operations (like fused multiply-add) that the user may call explicitly.

Floating point arithmetic is nasty business.
The mathematician \href{https://en.wikipedia.org/wiki/Alston_Scott_Householder}{Alston Householder} once said that he
  ``would never fly in an aircraft that had been designed with the help of floating point arithmetic''.\footnote{\url{http://homepages.gac.edu/~holte/talks/talk-flp.pdf}}
Modern planes are now \emph{flown} using floating-point arithmetic.
Let's hope those programs were compiled with CompCert.


\subsection{Verified Parsing}
\label{sec:menhir}

An important part of CompCert's front-end is converting C99 source code to an abstract syntax tree.
This task is done by a parser created with the \href{http://gallium.inria.fr/~fpottier/menhir/}{Menhir} parser generator.
Using a parser generator is important because parser automaton are often very large and difficult to read (CompCert's is \href{https://github.com/AbsInt/CompCert/blob/master/cparser/Parser.vy}{over 60,000 lines})
 but the parser's spec should ideally be small enough to stand manually validation against a reference like the C99 standard.

Menhir now guarantees that the parsers it generates accept the same language as the input grammar that generates them~\cite{jpl-validating}.
The proof is by translation validation: before declaring a successful parse, Menhir validates the result AST against the specification grammar.
There is still a danger that the grammar does not recognize the C99 language, but the specification grammar is at least easier to manually check than the full parser.
Moreover, the Menhir parser is reasonably efficient, only 3x slower than an unverified parser.\footnote{\url{http://gallium.inria.fr/blog/verifying-a-parser-for-a-c-compiler/}}
The net affect on compilation is within a 20\% slowdown~\cite{jpl-validating}.

%% JHJ says:
%% - C is not LR(1)
%% - original implementation hella slow (hashing, dynamic checking inductives)
%% - no Menhir frontend, had to write Coq
%% http://gallium.inria.fr/blog/verifying-a-parser-for-a-c-compiler/
%% Did they fix this? I cna't tell from reading source comments

